# This is a Bibtex reference
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:t arch:headline ^:nil
<<<<<<< HEAD
#+OPTIONS: author:t broken-links:nil c:nil creator:nil
#+OPTIONS: d:(not "LOGBOOK") date:nil e:nil email:nil f:t inline:t num:t
#+OPTIONS: p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t
#+OPTIONS: timestamp:nil title:t toc:t todo:t |:t
#+TITLE: seis-ml-api中間レポート2
=======
#+OPTIONS: author:t broken-links:nil c:nil creator:t
#+OPTIONS: d:(not "LOGBOOK") date:nil e:nil email:nil f:t inline:t num:t
#+OPTIONS: p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t
#+OPTIONS: timestamp:nil title:t toc:t todo:t |:t
#+TITLE: seis-ml-api中間レポート(再編)
>>>>>>> 8b06f277a20cc3d6e2ac8260de58bd4f3f6fca06
#+DATE: 
#+AUTHOR: 情報科学類二年 江畑 拓哉(201611350)
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
<<<<<<< HEAD
#+CREATOR: Emacs 24.5.1 (Org mode 9.0.1)
=======
#+CREATOR: Emacs 25.2.1 (Org mode 9.0.9)
>>>>>>> 8b06f277a20cc3d6e2ac8260de58bd4f3f6fca06
#+LATEX_CLASS: koma-article
#+LATEX_CLASS_OPTIONS: 
#+LATEX_HEADER_EXTRA: \bibliography{reference}
#+LaTeX_CLASS_OPTIONS:
#+DESCRIPTION:
#+KEYWORDS:
#+SUBTITLE:
#+STARTUP: indent overview inlineimages

<<<<<<< HEAD
* 情報特別演習概要
　本演習は江畑、栗本、畑中の3名により実施する。それぞれ演習を進め、最終的に大規模データベースを用いた機械学習API（seis-ml-api）を作成することが目標である。

** 演習範囲に関して
　以下に示す範囲を演習し、その成果を組み合わせることでseis-ml-apiの作成を目指す。

  #+CAPTION:演習範囲に関して
=======
* 前書き
  　このレポートは``江畑拓哉個人''が作成した情報特別演習における中間レポートである。ここで記載されているものは、現在までの成果を出した進捗に限り、それまでの失敗した試行などについては記載されていない。
  　また既存のツールを用いて実験した結果や、機械学習やデータベースの細かいプログラムについては完成次第別紙に実験レポートに作成する。
  　最後にこのレポートに関する意見などに関しては混乱を避けるため、できうる限り直接ご指摘頂ければ幸いである。
* seis-ml-api概要
  　この api (と仮定する)は、この情報特別演習をグループで受講した我々（江畑,栗本,畑中）の最終的な目標である。我々はまず各々の学ぶ分野についての理解を深め、その成果を適宜持ち寄ってこれを作成する予定になっている。

  #+CAPTION: 主な役割について（但し互いに柔軟に協力し合う）
>>>>>>> 8b06f277a20cc3d6e2ac8260de58bd4f3f6fca06
  #+ATTR_LATEX: :environment tabular :align |c|c|c|
|------+----------------+--------------------------------------|
| 氏名 | 分野           | 内容                                 |
|------+----------------+--------------------------------------|
<<<<<<< HEAD
| 江畑 | データベースと機械学習の統合 | 大規模データを利用した機械学習の作成 |
|------+----------------+--------------------------------------|
| 栗本 | 機械学習       | 機械学習モデルの調整                       |
|------+----------------+--------------------------------------|
| 畑中 | データベース   | 大規模データベースの作成             |
|------+----------------+--------------------------------------|

** seis-ml-api概要
　seis-ml-apiは、機械学習を提供するWeb APIである。大規模データベースを採用する。
　seis-ml-apiのフローを（TODO: Figure 1）に示す。

#+CAPTION: seis-ml-api フロー
#+ATTR_LATEX: :width 15cm
[[./idea-0-1.png]]

　ユーザはAPIに自分の持っているデータを登録する。APIは、APIに登録されているデータを用いて機械学習処理を行い、その結果をユーザに返す。

** 実験に用いるデータ
   　以下のデータをデータベースに登録し、実験段階においても用いたいと考えている。
      データの相関を求める関係上、これらの入手元から日本国内の経済についてのデータを集めたいと考えている。

  - Quandl
    　ほとんどすべてのデータはここで手に入る。ただし、長期間のデータは乏しいようである。主には、ここから得た株価データを用いて分析を行う予定である。
  - google finance 
      日本のデータをcsv形式で入手することは困難だが、海外のデータは容易に手に入る。
  - 総務省統計データ
    　めぼしいデータは少ないが、ゼロではないため活用していきたい。

* それぞれの進捗について
  - 江畑
    以下に引用されている文書を学習し、その結果より機械学習アルゴリズムの策定を行った。それに加え Java 、Python 、Clojure での Hbase の利用方法について学習した。
  - 栗本
    「機械学習」の履修及び関連書籍の学習を行った。また、主専攻実験「ヒューマンセンシング」の自主実験において、サポートベクターマシンを用いた簡易画像分類器を作成した。
  - 畑中
    HBase 、Hadoop を用いて疑似分散環境を構築した。また、JavaからHBaseにアクセスする方法を、実際にプログラムを動作させて確認した。


* 今後の演習について
- 江畑
策定した機械学習のモデルを実際のコードに実現する作業とHBaseに入力されたデータを送るAPIを作成する。
- 栗本
機械学習モデルの理解を深め、Pythonによるデータ分析に慣れ、より良いモデル調整が可能なように学習していきたいと考えている。
- 畑中
　完全分散環境を構築して、HBaseの性能テストを行いたいと考えている。


=======
| 畑中 | データベース   | 大規模データベースの作成             |
|------+----------------+--------------------------------------|
| 栗本 | 機械学習       | 機械学習の理解                       |
|------+----------------+--------------------------------------|
| 江畑 | 上記２つの統合 | 大規模データを利用した機械学習の作成 |
|------+----------------+--------------------------------------|

  　つまり、我々の情報特別演習のゴールは二段階あり、一段階はそれぞれの学習分野の習得、二段階はそれらを持ち寄ってこの seis-ml-api という何かの作成を行う、というものである。
  　問題となる api の詳細についてだが、機械学習が何を返すことができるものであるのかを全員が共有できていないため、詳細な情報を明記することはできない。しかし目標を理解しやすくするため、無理にこれについて個人的な見解に基づいた説明を行う。
  　この api では以下の抽象的なチャートに基づいた設計を行う。

  #+CAPTION: seis-ml-apiの抽象的チャート
  #+ATTR_LATEX: :width 15cm
  [[./idea-1.png]]
  
  　ここで注意しなければならないのは、この案に関してはデータベースに Hbase を利用するという点以外に関しては江畑が独断で学習した成果によって作成されたものであり、機械学習分野のこれからの成果次第では作成されるものも、またこの制作物の利用法も十分に変わり得るということである。
  　その前提に於いて、現在までのこの物の利用価値について説明するならば、恐らくこの api は``大量の時系列データを利用した価格予測api''と呼べるものなのではないだろうか。ユーザから時系列データを受取り、データベースに保存されたデータと関連付けて機械学習を行い、ユーザから指定された時点の予測値や相関の強いデータの名称を返すというのが現時点での江畑の理解である。
  　そしてこの構成要素は、大きく分けて機械学習とデータベースの二つである。
  　データベースに関しては畑中氏が作成している大規模データを扱うことのできる HBase を用いる予定である（詳細は彼と彼の担当の教員に尋ねて頂きたい）。江畑はこのデータベースに入手したデータ、それを予測したデータ、後述する SARIMA 予測をする際に用いるモデルなどを入れるつもりである。
  　機械学習部分では、まずデータベースから問題となる時系列データについて関連性のあるデータを読み出し、それらについて時系列予測を行う。そしてそれらを元にして問題となった時系列データとの相関を調べ、いわば欠損値となっているユーザから指定された時点での値を求める。そしてその際に求まる相関の強かった関連データの名称も合わせてユーザに返すことがこの部分での実装予定のものである。
  　
  　これだけでは抽象的で理解が難しいため、江畑の考えている動作の流れを以下に示す。
  　例えばユーザから、現在までの時系列データがそれの所属するカテゴリ付きで渡されたとする。まずデータベース側から、指定されたカテゴリに関するデータが機械学習側に渡される。機械学習側ではそれらをユーザから指定された時点（例えば一週間後）まで時系列予測する。次に、ユーザから渡されたデータに対して先ほど予測したデータ群との相関を求めていく。その相関を用いて指定された時点の値を予測する。結果としてその値と相関の強かったデータの名称群を返す。最後にユーザから渡されたデータをデータベース側に保存して一連の動作は終了となる。

  つまり必要となるデータは、
  - タイトル
  - 時系列データ
  - そのデータの属するカテゴリ
  - 欲しい時点についての情報

　そして返すデータは、
  - 予測値
  - 相関についてのデータ

　ということになる。

** 実験に用いるデータ
   　ここでは、初期にデータベースに入っているデータとして挙げられ、なおかつ実験段階に於いて使用できると考えられるデータの入手元について紹介する。江畑個人の考えとしては、データの相関を求める関係上、これらの入手元から日本国内の経済についてのデータを集めて行きたいと考えている。
  - google finance 
      日本のデータをcsvで入手することは困難だが、海外のデータは容易に手に入る。
  - Quandl
    　ほとんどすべてのデータはここで手に入る。但し、どうやら長期間のデータは乏しいようである。主にはこちらから得た株価データを用いて分析を行う予定である。
  - 総務省統計データ
    　あまりめぼしいデータはないが、ゼロというわけではないため活用していきたいと考えている。

* 機械学習部分（時系列解析）
    この章に関する内容は全て江畑個人の報告であり、他のメンバーの活動に何ら影響を与えるものではない。
  　実験に関しては、別紙にまとめて示す。(仮決定のこの部分のみの実験データは同フォルダのreport.ipynbである)
  　時系列解析に用いるモデルは、季節的自己回帰和分移動平均モデルことSARIMAモデルを用いる予定である。
  　SARIMAモデルとは、三つの要素が重ね合わさったモデルである。
  　まず、ARという部分は Autoregressive を表し、これは自己回帰を意味する。自分の以前の観測データに対して重回帰分析を行うものである。例えば、 $a->b->c$ という遷移があれば、 $b->c->d$ といったことを考えられることに似ている。
  　MAというのは Moving average 、つまり移動平均を意味している。移動平均とは、ある区間 [a, b] の平均値と b 或いは a などと比較する際に用いられる言葉のようで、平均値を平均の位置ではない別の位置の値と比較することを意味している。
  　ARIMAというのは、以上の２つを組み合わせるという意味である。
  　そしてSというのは、Seasonalというのは、季節性という意味で、ある期間の周期性を用いるということである。これは例えば毎年同じような活動をするものに対して非常に有効な手段であるようで、 ARIMA モデルの拡張の１つとして広く認められているようである。

* 機械学習部分（相関解析）
  　この章に関する内容は全て江畑個人の報告であり、他のメンバーの活動に何ら影響を与えるものではない。
  　実験に関しては別紙にまとめて示す予定である。
  　概要で紹介したように、関連データについての時系列解析が終わった後に行う処理がこの相関関係を解析する部分である。江畑はここではランダムフォレストの回帰を用いた解析のうちの１つ、欠損値補完を行いたいと考えている。ランダムフォレストの大まかなアルゴリズムは以下で紹介する決定木の低いものを多く生やすことでデータの分析を行うもので、特に今回は回帰木を用いる。

** 決定木
   　決定木とは、複数の説明変数を持つデータセットに対して、最も議論のデータセットを分割できるように境界を設け、そこで分割されたそれぞれのデータセットに対して同様の処理を繰り返していくことで、データの特徴を抽出していく機械学習の手法の１つである。データセットの分割に用いられる指標として、尤離度（逸脱度）やジニ係数、エントロピーなどを挙げることができる。またここで用いる決定木の高さとは、あるデータに対してどの程度分割処理を行ったか、というものである。そして分割数が多いものは高い決定木、分割数か少ないものは低い決定木と呼ぶこととする。また当然のことながら、決定木は低ければ大まかな予測が可能であり、高い場合には精度は上がるものの、過学習を起こす可能性もある。
** ランダムフォレスト
   　ランダムフォレストとは、与えられたデータセットの中から任意に抽出して集めたデータセットを複数作り、それぞれに低い決定木を用いた学習を行い、結果を集計することで元のデータセットの分析を行うという仕組みのことである。今回の回帰を用いた欠損補完においては、それぞれの決定木が求めた値の平均を取ることで欠損値補完を行う。そして、決定木で学習しなかった残りのデータを用いて説明変数の重要度を分析する。

* データベース部分
  　この章に関する部分のうちデータベースの選択、作成に関しては畑中の貢献によるものであり、江畑は何も関与していない。データベースの利用方法については江畑が独自に行ったものであり、他のメンバーとは共有していない事項である。
  　データベースの作成部分に関してはApach Hbaseを用いた大規模スケールの箱を作る予定である。大規模データベースの中身の詳細な設計については江畑の理解が追いつくものでもなく、機械学習の手法次第では挿入するデータに大きな変化がある可能性があるが、畑中の報告を伺う限りでは数TB程度の完全分散システムにするとのことであった。
  　データベースの利用については、PythonとClojureでのHbaseの利用方法についての学習を行った。しかし前者はしっかりとしたモジュールがあったが、後者は自信をもって選択できるものがなかった。そのため、この言語の特性を活かしてJavaからの利用を目指し、できるならば自作の独自のクエリ（例えばｃｓｖ読み込み）などを実装したモジュールを作成したいと考えている。
>>>>>>> 8b06f277a20cc3d6e2ac8260de58bd4f3f6fca06

* 参考文献
  以下にそれぞれで用いた参考文献を示す。なお、これらの文献は今後より深く読み進めていく予定である。
  - SARIMAモデルについて \cite{sarima01} \cite{sarima02} \cite{sarima03} \cite{sarima04} \cite{sarima05}
  - RandomForestについて \cite{rf01} \cite{rf02} \cite{rf03} \cite{rf04} \cite{rf05} \cite{rf06} \cite{rf07} \cite{Breiman:2001:RF:570181.570182} \cite{rf08} \cite{rf09}
  - 決定木について \cite{tree01}
  - データベースについて \cite{hbase01} \cite{hbase02} \cite{hbase03}
  - その他 \cite{bunpou01}

\printbibliography
