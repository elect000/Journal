@online{cnnsa,
Author = {Yoon Kim},
Title = {Convolutional Neural Networks for Sentence Classification},
Year = {2014},
Eprint = {1408.5882},
Eprinttype = {arXiv},
}
@online{fasttext,
Author = {Armand Joulin and Edouard Grave and Piotr Bojanowski and Tomas Mikolov},
Title = {Bag of Tricks for Efficient Text Classification},
Year = {2016},
Eprint = {1607.01759},
Eprinttype = {arXiv},
}
@online{word2vec,
Author = {Yoav Goldberg and Omer Levy},
Title = {word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method},
Year = {2014},
Eprint = {1402.3722},
Eprinttype = {arXiv},
}
@online{copynet,
Author = {Jiatao Gu and Zhengdong Lu and Hang Li and Victor O. K. Li},
Title = {Incorporating Copying Mechanism in Sequence-to-Sequence Learning},
Year = {2016},
Eprint = {1603.06393},
Eprinttype = {arXiv},
}

@InProceedings{seq2bseq,
  title = 	 {Sequence to Better Sequence: Continuous Revision of Combinatorial Structures},
  author = 	 {Jonas Mueller and David Gifford and Tommi Jaakkola},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {2536--2544},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {8},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/mueller17a/mueller17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/mueller17a.html},
  abstract = 	 {We present a model that, after learning on observations of (sequence, outcome) pairs, can be efficiently used to revise a new sequence in order to improve its associated outcome. Our framework requires neither example improvements, nor additional evaluation of outcomes for proposed revisions. To avoid combinatorial-search over sequence elements, we specify a generative model with continuous latent factors, which is learned via joint approximate inference using a recurrent variational autoencoder (VAE) and an outcome-predicting neural network module. Under this model, gradient methods can be used to efficiently optimize the continuous latent factors with respect to inferred outcomes. By appropriately constraining this optimization and using the VAE decoder to generate a revised sequence, we ensure the revision is fundamentally similar to the original sequence, is associated with better outcomes, and looks natural. These desiderata are proven to hold with high probability under our approach, which is empirically demonstrated for revising natural language sentences.}
}
@online{seq2seq,
Author = {Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
Title = {Sequence to Sequence Learning with Neural Networks},
Year = {2014},
Eprint = {1409.3215},
Eprinttype = {arXiv},
}
@article{twitercleaning,
  title={Twitterを用いた非タスク指向型対話システムのための発話候補文獲得},
  author={稲葉 通将 and 神園 彩香 and 高橋 健一},
  journal={人工知能学会論文誌},
  volume={29},
  number={1},
  pages={21-31},
  year={2014},
  doi={10.1527/tjsai.29.21}
}
@article{dae,
  title={Extracting and composing robust features with denoising autoencoders.},
  author={P. Vincent and H. Larochelle and Y. Bengio and P.-A. Manzagol.},
  howpublished={In Proceedings of the 25th International Conference},
  year={2008},
}
@online{scdv,
Author = {Dheeraj Mekala and Vivek Gupta and Bhargavi Paranjape and Harish Karnick},
Title = {SCDV : Sparse Composite Document Vectors using soft clustering over distributional representations},
Year = {2016},
Eprint = {1612.06778},
Eprinttype = {arXiv},
}