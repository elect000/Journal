#    -*- mode: org -*-


Archived entries from file /home/meguru/Github/Journal/skn/last-report/report.org


* abstract 
  :PROPERTIES:
  :ARCHIVE_TIME: 2018-12-30 日 02:25
  :ARCHIVE_FILE: ~/Github/Journal/skn/last-report/report.org
  :ARCHIVE_OLPATH: 序論/研究背景及び目的
  :ARCHIVE_CATEGORY: report
  :END:
　まず受付やオンラインチャットなどにおいて対話システムの需要が増えていること、Amazon AlexaやSiriなどを例に出して説明する。次にキャラクタ性を持ったマルチモーダル対話システムとして、りんなを例に上げる。
　その実装例として、Amazon Alexa Prize のコンテストを例に出す。
　本研究の目的として、日本語環境下で、りんなのような機能を持つシステムを構築すること、ゲームAIへの転用などを視野にいれていることを説明する。そしてシステムの概要として、対話システムという大問題に対して、いくつかの小問題に分割し、それらを組み合わせるモジュール分割という手法を用いることにしたことを説明する。
[fn:rinna] https://twitter.com/ms_rinna
[fn:alexaprize] https://developer.amazon.com/alexaprize
[fn:hred] HRED (\cite{1507.02221}) や VHRED (\cite{1605.06069}) があるが、発話の多様性を得ること(一般的な受け答えを学んでしまい、同じような文ばかり生成してしまうこと)やデータを十分に集めることが難しいなど課題がある。
[fn:multimordule] 日本で人気を得ている ``マルチモーダルエージェントAI'' とは、複数のソースから問題を見直すという特徴があるが、これは複数のモデルを使っているという意味で同じではあるが、問題を分割しようとしているわけではないという点でこの研究と大きく違うと言えるだろう。
[fn:transformer] 2017年12月時点

* Skip-gram
  :PROPERTIES:
  :CUSTOM_ID: skip-gram
  :ARCHIVE_TIME: 2019-01-22 火 15:16
  :ARCHIVE_FILE: ~/Github/Journal/skn/last-report/report.org
  :ARCHIVE_OLPATH: 日本語データの取り扱いについて/関連研究
  :ARCHIVE_CATEGORY: report
  :END:
Skip-gram (\cite{Mikolov2013DistributedRO})のアルゴリズムは以下(\ref{tab:Skip-algo})のとおりである。[fn:neg-sample]
\begin{itembox}[l]{Skip-gram のアルゴリズム}
\label{tab:Skip-algo}
1. 正のサンプルとして、ターゲットの単語とその周辺の単語を取り出す。\\
2. 負のサンプルとして、単語辞書の中からランダムにサンプルされた単語を取り出す。\\
3. ロジスティックス回帰を用いてこの2つのサンプルを区別できるようにネットワークを訓練する。\\
4. ネットワークの重みを単語埋め込みとみなす。
\end{itembox}
#+ATTR_LATEX: :width 10cm
#+CAPTION: Skip-gram は文中におけるある単語の周辺単語を予測する (w(t)は t番目の単語を示す。) (\cite{NIPS2013_5021} より)
#+NAME: fig:
[[./img/skip-gram.PNG]]
#+CAPTION: fasttext の Skip-gram を用いた単語分散獲得学習のパラメータ
#+ATTR_LATEX: :environment longtable :align |c|c|
|--------------------------+--------------------------------------------------------------|
| パラメータ名             | 説明                                                         |
|--------------------------+--------------------------------------------------------------|
| 許容最低語彙頻度         | 語彙として認める単語の頻度。                                 |
|                          | これを下回る単語は頻度の少ない単語として学習の対象としない。 |
| 学習係数                 | 目的関数 Adagrad の学習係数。                                |
| 学習係数向上率           | 学習率の更新率、単語がこの数だけ訓練されると更新される。     |
| epoch 数                 | 語彙の数 に対して何倍訓練を行うかを決定する。                |
| ネガティブサンプリング数 | 学習ごとに負のサンプルをどのくらい抽出するか。               |
| ウィンドウサイズ         | アルゴリズムで説明した m の値                                |
| 損失関数                 | 損失関数                                                     |
| dim                      | 埋め込みベクトルの次元数                                     |
|--------------------------+--------------------------------------------------------------|

ここで fasttext で用いられている subword との関連について説明する。まず Skip-gram の損失関数を以下の条件のもとで示すと以下のようになる。
1. Skip-gram で予測する単語はある単語の前後一単語のみ。
2. 単語を $w_i$ 、コーパスを $[w_1, \cdots, w_T]$ とする。
3. ネガティブサンプルの手続きを省く。
4. 語彙数は W とする。
\begin{eqnarray}
L &=& - \cfrac{1}{T}\Sigma^T_{t=1}(logP(w_{t-1}, w_{t+1}|w_t)) \notag \\
  &=&- \cfrac{1}{T}\Sigma^T_{t=1}(logP(w_{t-1}|w_t) + logP(w_{t+1}|w_t))
\end{eqnarray}
　この際に通常のSkip-gram では $P(w_c|w_t)$ は以下の式で表される。
\begin{eqnarray}
P(w_c|w_t) = \cfrac{e_{s(w_t, w_c)}}{\Sigma^W_{j=1}e^{s(w_t, j)}}
\end{eqnarray}
　問題はこの内の関数 $s$ だ。
[fn:neg-sample] 計算の都合上、辞書全体の単語を取り上げることが不可能なため、ネガティブサンプリングを行っている。またこのサンプリングは均一ではなく、高頻度な単語は程よく省かれるようになっている。(\cite{NIPS2013_5021})


* 深層学習の基礎知識
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-01-22 火 23:19
  :ARCHIVE_FILE: ~/Github/Journal/skn/last-report/report.org
  :ARCHIVE_OLPATH: 付録
  :ARCHIVE_CATEGORY: report
  :END:
** CNN
CNN(Convolutional neural network (\cite{fukushima:neocognitronbc})/\cite{LeCun:1999:ORG:646469.691875}) とは格子型のトポロジーを持つデータを解析するためのニューラルネットワークの一種だ。例えばそれは二次元な(RGB 8bit)デジタル画像 ($\mathbb{N}'^{x \times y}\ \mathbb{N}' \in \{0, 2, \dots, 254\}$)、或いは一単語をベクトル $x_i\in\mathbb{R}^{x\times 1}$ としたときの文章ベクトル $[x_1, \dots, x_n] \in\mathbb{R}^{x\times n}$ を挙げることが出来る。\\
　ここでいう畳み込みとは、線形変換の一種であり、以下の式(この式は一次畳み込み)で表されるものを指している。
\begin{eqnarray}
s(t) &=& \int x(a)w(t-a) da \notag \\
&=& (x\star w)(t)
\end{eqnarray}
CNN において $x$ はデータの多次元配列を示しており、wはカーネルと呼ばれる学習されたパラメータの多次元配列を示すこととなる。\\
　より実計算に近づけるため、これを2次元実空間上の離散問題として再定義すると、多次元配列を $I\in\mathbb{R}^{M\times N}$ 、カーネルを $K\in\mathbb{R}^{m\times n}$ 、バイアスを $B\in\mathbb{R}^{M-m\times N-n}$ 、出力を $S\in\mathbb{R}^{M-m\times N-n}$ として以下の式に変換される。(正確にはこの手続きの途中で畳み込みを相互相関(Cross-correlation)と解釈し直している(t-a $\rightarrow$ t + a)が、 $a, b$ の符号を反転させれば同様のことが言える。)
\begin{eqnarray}
S_{i,j} &=& B + \Sigma_{a=0}^{m-1}\Sigma_{b=0}^{n-1}I_{i, j}K_{i+a,j+b} \notag \\
&=&B + \Sigma_{a=0}^{m-1}\Sigma_{b=0}^{n-1}K_{i, j}I_{i+a,j+b}
\end{eqnarray}
　このとき誤差関数を $E$ としてパラメータ $K, B$ について勾配を求めると、
\begin{eqnarray}
\cfrac{\partial E}{\partial K} &=& \Sigma_{i=0}^{M-m}\Sigma_{j=0}^{N-n}\cfrac{\partial E}{\partial S_{i,j}}\cfrac{\partial S_{i,j}}{\partial K_{i,j}}\notag \\
&=&\Sigma_{i=0}^{M-m}\Sigma_{j=0}^{N-n}\cfrac{\partial E}{\partial S_{i,j}}I_{i+a, j+b}\\
\cfrac{\partial E}{\partial B} &=& \Sigma_{i=0}^{M-m}\Sigma_{j=0}^{N-n}\cfrac{\partial E}{\partial S_{i,j}}\cfrac{\partial S_{i,j}}{\partial B_{i,j}}\notag \\
&=&\Sigma_{i=0}^{M-m}\Sigma_{j=0}^{N-n}\cfrac{\partial E}{\partial S_{i,j}}
\end{eqnarray}
　この勾配を用いて逆伝搬を行い学習を行うことになる。例としてSGD(Stochastic gradient decent) を用いて $K$ を更新する手続きを考えると 、ステップ数を $t$ 、学習係数を $\eta$ として以下のようになる。
\begin{eqnarray}
K^{(t+1)} = K^{(t)} - \eta \cfrac{\partial E}{\partial K^{(t)}}
\end{eqnarray}
　また一般に CNN は一般に活性化関数についてもまとめられることがあり、本研究でもこの2つをまとめている。活性化の関数については sigmoid 関数や relu 関数、elu関数などがある。\\
　尚本研究の CNN の文脈で登場するフィルターサイズとは以上のシーケンスを行う数で、例えばある画像に対しフィルターサイズ 128 の畳み込みを行う、というのは 128枚の同じサイズの出力結果(それぞれの出力結果が等しいとは限らない)が得られるということを示している。
** 活性化関数
活性化関数は非線形関数の一種で、例えば sigmoid(logistic sigmoid) 関数、relu(rectified linear unit) 関数(\cite{Nair:2010:RLU:3104322.3104425})、elu(exponential linear unit) 関数(\cite{Clevert2015FastAA})を挙げることが出来る。
\begin{eqnarray}
sigmoid(z) &=& \cfrac{1}{1 + e^{-z}}\\
relu(z) &=& \max\{0, z\}\\
elu(z) &=& 
\begin{cases}
z\ &(z < 0)\\
e^x - 1\ &(z \leq 0)
\end{cases}
\end{eqnarray}
** 最大プーリング
最大プーリング(層)とはプーリング(層)の一種である。プーリングを行うプーリング関数とは特定の場所のネットワークの出力を周辺の出力を最大値や平均値といった要約統計量で置き換える処理を行う関数だ。例えば最大プーリング層は最大値を要約統計量として用いるプーリング関数を採用したプーリング層で、平均プーリング層は平均値を要約統計量として用いるプーリング関数を採用したプーリング層だ。\\
　更にこれを発展させるものとして、ストライドと呼ばれる場所の特定の指定幅を調節する(標準では 1 、このとき特定の場所は前の特定の場所の一つ隣を示す。)場合もある。
** RNN
RNN(reccurent neural network (\cite{Jain:1999:RNN:553011})) とは時系列データを解析するためのニューラルネットワークの一種だ。これを簡潔に紹介するために、パラメータ $\theta$ を持ち $t-1$ の状態 $h$ を外部入力 $x_t$ を伴って $t$ へ遷移させる $f$ という関数を用いる、以下のシステムを考える。
\begin{eqnarray}
h^{(t)} = f(h^{(t-1)};x^{(t)};\theta)
\end{eqnarray}
　一般に RNN ではこのシステムをモデル化していると言える。つまりRNNは入力データ $\{x^{1}, \cdots, x^{\tau}\}$ をある関数 $f$ を繰り返し適用しているとも言い直すことが出来る。\\
　また本研究中で登場した双方向 LSTM はこの入力データを反転したもの $\{x^{\tau}, \cdots, x^{1}\}$ も考慮しようというモチベーションに基づいている。[[[fig:birnn][Figure 12]]]\\
　もう少し数学を用いて説明するため、標準的なRNNの構造を示す[[[fig:rnn][Figure 11]]]。
#+CAPTION: 標準的な RNN の構造 (\cite{Goodfellow2016c10} より)
#+NAME: fig:rnn
#+ATTR_LATEX: :width 15cm
[[./img/rnn.PNG]]
　
右は左を時系列的に展開した図だ。各変数は以下のような意味を持っている。
- x
  外部入力で、一般にベクトルが用いられる。
- U
  外部入力を状態へ反映させるための重み行列を示している。
- h
  状態を示しており、一般にベクトルが用いられる。隠れ状態とも呼ばれる。
- W
  隠れ状態を更新する重み行列を示している。
- V
  隠れ状態から出力を計算する重み行列を示している。
- o
  出力で、一般にベクトルが用いられる。
- L
  損失で、モデルからの出力と正解との差(厳密な距離である必要はない)を示す。
- y
  正解を示しており、o と同じもの(例えばベクトル)だ。

この構造についていくつかの式を定義することが出来る。尚この式で登場する b, c はバイアスベクトルを示している。
\begin{eqnarray}
a^{(t)} = b + W h^{(t-1)} + Ux^{(t)}\\
h^{(t)} = tanh(a^{(t)})\\
o^{(t)} = c + Vh^{(t)}\\
\hat{y}^{(t)} = softmax(o^{(t)})
\end{eqnarray}

これらの式を繰り返し適用していくことで RNN は更新されていくことになる。\\
次に損失について求める。損失は外部入力と正解を用いて、次の式で求められる。尚、ここで言う $p_{model}(y^{(t)}|\{x^{(1)}, \cdots, x^{(t)} \})$ は 出力ベクトル $\hat{y}^{(t)}$ における $y^{(t)}$ の確率を考えているものとする。
\begin{eqnarray}
L({x^{(1)}, \cdots, x^{(\tau)}}, {y^{(1)}, \cdots, y^{(\tau)}}) &=& \Sigma_t L^{(t)} \\
&=& - \Sigma_t log p_{model}(y^{(t)}|\{x^{(1)}, \cdots, x^{(t)} \})
\end{eqnarray}
勾配の計算については定義しなければならない要件が多いため、参考文献 (\cite{Goodfellow2016c10}) を参照して頂きたい。

#+CAPTION: 双方向 RNN の構造 (\cite{Goodfellow2016c10} より)
#+NAME: fig:birnn
#+ATTR_LATEX: :width 15cm
[[./img/birnn.PNG]]
** LSTM
LSTM(long short-term memory) とは RNN が長期的な系列の性質を保持しづらいという問題を解決する手法の一つ、自己ループ(内部回帰)を持つ ``LSTMセル''という構造を加えた RNN の派生だ。LSTMセルの構造を[[[fig:lstm][Figure 13]]] に示す。特に $\int$ のような記号はシグモイド関数を示しており、$+$ は加算、 $x$ は乗算を示している。\\
　RNNとの差異を明確にするため、h の更新式を示していく。まずLSTMセルの input について説明する。\\
　input はバイアスベクトル $b$ と入力に対する重み行列 $U$ 、隠れ状態を更新する重み行列 $W$ を用いて以下の式で求められる。
\begin{eqnarray}
input^{(t)}_i = sigmoid(b_i + \Sigma_j U_{i, j} x_j^{(t)} + \Sigma_j W_{i, j}h_j^{(t-1)}) 
\end{eqnarray}
　input gate も同様に、バイアスベクトル $b^g$ と入力に対する重み行列 $U^g$ 、隠れ状態を更新する重み行列 $W^g$ を用いて以下の式で求められる。
\begin{eqnarray}
g^{(t)}_i = sigmoid(b_i^g + \Sigma_j U_{i, j}^g x_j^{(t)} + \Sigma_j W_{i, j}^g h_j^{(t-1)}) 
\end{eqnarray}
以下 forget gate、 output gate も同様に、
\begin{eqnarray}
f^{(t)}_i = sigmoid(b_i^f + \Sigma_j U_{i, j}^f x_j^{(t)} + \Sigma_j W_{i, j}^f h_j^{(t-1)}) \\
o^{(t)}_i = sigmoid(b_i^o + \Sigma_j U_{i, j}^o x_j^{(t)} + \Sigma_j W_{i, j}^o h_j^{(t-1)}) 
\end{eqnarray}
　以上の式からLSTMセルの状態 $s$ についての更新式は以下のように書ける。
\begin{eqnarray}
s^{(t)}_i = f_i^{(t)}s_i^{(t-1)} + g_i^{(t)} input_i^{(t)}
\end{eqnarray}
　そしてこのLSTMセルの状態と output gate から LSTMセルの出力である $h_i^{(t)}$ が求められる。
\begin{eqnarray}
h^{(t)}_i = tanh(s_i^{t})o^{(t)}_i
\end{eqnarray}
#+CAPTION: LSTM セルの構造 (\cite{Goodfellow2016c10} より)
#+NAME: fig:lstm
#+ATTR_LATEX: :width 15cm
[[./img/lstm.PNG]]
#+LATEX: \newpage
** 最適化関数
最適化関数とは、求められた損失に基づいてそれが小さくなるように重みを更新するための関数だ。例えばSGD (確率的勾配降下法 Stochastic Gradient Descent) や Adam といった種類が存在する。\\
　SGDのアルゴリズム(\cite{Goodfellow2016c10} より)を以下に示す。補足すると SGD の収束を保証する学習率の十分条件は以下のようになる。
\begin{eqnarray}
\Sigma_{k=1}^{\infty}\epsilon_k = \infty \ and \ \Sigma_{k=1}^{\infty} \epsilon_k^2 < \infty
\end{eqnarray}

\begin{algorithm}
\caption{SGD の アルゴリズム}
\label{sgd}
\begin{algorithmic}
\REQUIRE 学習率 $\epsilon_1 , \epsilon_2 , \dots$
\REQUIRE 初期パラメータ $\theta$
\STATE $k\leftarrow 1$
\WHILE{終了条件を満たさない}
\STATE 訓練データ ${x^{(1)}, \dots , x^{(m)}}$ と対応する目標 $y^{(i)}$ の m 個の事例を集めたミニバッチをサンプリングする
\STATE 勾配の推定値を計算する: $\hat{g}\leftarrow \cfrac{1}{m}\nabla_{\theta}\Sigma_i L(f(x^{(i)}; \theta), y^{(i)})$ 
\STATE パラメータの更新を行う: $\theta \leftarrow \theta - \epsilon_k \hat{g}$
\STATE $k \leftarrow k+1$
\ENDWHILE
\end{algorithmic}
\end{algorithm}
　Adam のアルゴリズム(\cite{Goodfellow2016c10} より)を以下に示す。
\begin{algorithm}
\caption{Adam の アルゴリズム}
\label{adam}
\begin{algorithmic}
\REQUIRE ステップ幅 (推奨値 $0.001$)
\REQUIRE モーメントの推定を行うための指数減衰率 $\rho_1$ 、$\rho_2$ $\in [0, 1)$ (推奨値 $0.9$ 、$0.999$)
\REQUIRE 数値的な安定のために使われる小さい定数 $\delta$ (推奨値 $10^{-8}$)
\STATE 一次・二次モーメントに関する初期値: $s=0$, $r=0$
\STATE 時間ステップの初期値: $t=0$
\WHILE{終了条件を満たさない}
\STATE 訓練データ ${x^{(1)}, \dots , x^{(m)}}$ と対応する目標 $y^{(i)}$ の m 個の事例を集めたミニバッチをサンプリングする
\STATE 勾配の推定値を計算する: $\hat{g}\leftarrow \cfrac{1}{m}\nabla_{\theta}\Sigma_i L(f(x^{(i)}; \theta), y^{(i)})$ 
\STATE $t\leftarrow t+1$
\STATE バイアス付きの一次モーメントの推定値を更新する: $s \leftarrow \rho_1 s + (1- \rho_1) g$
\STATE バイアス付きの二次モーメントの推定値を更新する: $r \leftarrow \rho_2 r + (1- \rho_2) g \odot g$
\STATE 一次モーメントのバイアスを修正する: $\hat{s} \leftarrow \cfrac{s}{1-\rho_1^t}$
\STATE 二次モーメントのバイアスを修正する: $\hat{r} \leftarrow \cfrac{r}{1-\rho_2^t}$
\STATE 更新量を計算する: $\Delta \theta = -\epsilon\cfrac{\hat{s}}{\sqrt{\hat{r}} + \delta}$ (この計算は要素ごとに計算される)
\STATE 更新量を適用する: $\theta \leftarrow \theta + \Delta \theta$
\ENDWHILE
\end{algorithmic}
\end{algorithm}


